{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109B Data Science 2: Advanced Topics in Data Science \n",
    "## Lecture 3 - Coding Environment Setup and review of `statsmodels`\n",
    "###  Notebook B\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Spring 2021**<br>\n",
    "**Instructors:** Mark Glickman, Pavlos Protopapas, and Chris Tanner<br>\n",
    "**Additional Instructor:** Eleni Kaxiras<br><BR>\n",
    "*Content:* Eleni Kaxiras and Will Claybaugh\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "* use `np.linalg.vander`\n",
    "* use the weird R-style formulas in `statsmodels`\n",
    "* practice least-squares regression in `statsmodels`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis Functions\n",
    "\n",
    "In our models we can use various types of functions as basis functions. Strictly speaking, in linear algebra where a basis for a subspace S of $\\mathbb{R}^n$ is a set of vectors that spans S and is linearly independent. As a reminder, a set of vectors $\\textbf{v}_1, \\textbf{v}_2, ..., \\textbf{v}_k$ are considered linearly independent if they cannot be written as a linear combination of each other, such that, if: $c_1\\textbf{v}_1+c_2\\textbf{v}_2+ ...+ c_k\\textbf{v}_k = \\textbf{0}$ then $c_1,c_2,...,c_k$ are all zero. <BR>\n",
    "    \n",
    "In data science where we have lots of imperfect data (with errors), as well as imperfect computers (with round-off errors), when we substitute their values into the matrices we almost always get column degeneracy, meaning, some of our columns become linear combinations of each other. Especially so if we use the monomial basis and go beyond ~5,6 degree of the polynomial. <BR>\n",
    "\n",
    "Examples are:\n",
    "\n",
    "- Monomials such as $x,x^2,x^4,x^5$ \n",
    "- Sigmoid/ReLU functions (neural networks)\n",
    "- Fourier functions \n",
    "- Wavelets \n",
    "- Splines\n",
    "    \n",
    "The matrix produced when we substitute the values of our data into the basis functions is called the *design matrix*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear/Polynomial Regression\n",
    "\n",
    "We will use the `diabetes` dataset.\n",
    "\n",
    "Variables are:\n",
    "- subject:   subject ID number\n",
    "- age:       age diagnosed with diabetes\n",
    "- acidity:   a measure of acidity called base deficit\n",
    "Response:\n",
    "- y:         natural log of serum C-peptide concentration\n",
    "\n",
    "*Original source is Sockett et al. (1987) mentioned in Hastie and Tibshirani's book \n",
    "\"Generalized Additive Models\".*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data in Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab = pd.read_csv(\"data/diabetes.csv\")\n",
    "diab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the design matrix for a fictitious dataset\n",
    "Let's keep just the `age` feature and create some columns of our own. Let's see how good this matrix is before we create the design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_age = diab[['age']].copy()\n",
    "diab_age['age2'] = diab_age.apply(lambda row: row['age']**2, axis=1)\n",
    "diab_age['random'] = np.random.normal(0,1,len(diab_age)) \n",
    "diab_age['same'] = diab_age['age']\n",
    "diab_age.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = diab_age.to_numpy(copy=True)\n",
    "A[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the columns of A are linearly independent by using some linear algebra methods from `numpy.linalg` and `sympy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import matrix_rank\n",
    "matrix_rank(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out which rows are linearly independent\n",
    "import sympy\n",
    "_, inds = sympy.Matrix(A).T.rref() \n",
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.cond(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the design matrix for `age` using a polynomial basis\n",
    "Let's keep just the `age` feature again and create the design matrix using a polynomial of degree `n`. First we will use the basic `numpy` formula `vander()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "vand = np.vander(diab_age.age, 2, increasing=True)\n",
    "vand[:3], vand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To our point why the Vandermonde matrix is usually ill-conditioned, \n",
    "## find the condition number of this matrix\n",
    "np.linalg.cond(vand), matrix_rank(vand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b>: Vandermonde matrix</div><BR>\n",
    "Change the degree of the polynomial and comment on what happens to the condition and rank of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "vand = np.vander(diab_age.age, 8, increasing=True)\n",
    "vand[:3], vand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To our point why the Vandermonde matrix is usually ill-conditioned, \n",
    "## find the condition number of this matrix\n",
    "np.linalg.cond(vand), matrix_rank(vand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear/Polynomial regression with statsmodels. \n",
    "\n",
    "As you remember from 109a, we have two tools for Linear Regression:\n",
    "- `statsmodels` [https://www.statsmodels.org/stable/regression.html](https://www.statsmodels.org/stable/regression.html), and \n",
    "- `sklearn`[https://scikit-learn.org/stable/index.html](https://scikit-learn.org/stable/index.html)\n",
    "\n",
    "Previously, in this notebook, we worked from a vector of target values and a design matrix we built ourself. In 109a we used e.g. `sklearn`'s PolynomialFeatures to build the matrix. Now we will look at `statsmodels` which allows users to fit statistical models using R-style **formulas**. They build the target value and design matrix for you. \n",
    "\n",
    "**Note:** Categorical features (e.g. let's say we had a categorical feature called Region, are designated by `C(Region)`), polynomial features (e.g. age) are entered as `np.power(age, n)` where `n` is the degree of the polynomial **OR** `np.vander(age, n, increasing=True)`.\n",
    "\n",
    "```\n",
    "# Example: if our target variable is 'Lottery', while 'Region' is a categorical predictor and all the others are numerical:\n",
    "df = dta.data[['Lottery', 'Literacy', 'Wealth', 'Region']]\n",
    "\n",
    "formula='Lottery ~ Literacy + Wealth + C(Region) + Literacy * Wealth'\n",
    "```\n",
    "\n",
    "For more on these formulas see:\n",
    "\n",
    "- https://www.statsmodels.org/stable/examples/notebooks/generated/formulas.html\n",
    "- https://patsy.readthedocs.io/en/latest/overview.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model1 = smf.ols('y ~ age', data=diab)\n",
    "fit1_lm = model1.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a dataframe to predict values on (sometimes this is just the test or validation set). Very useful for making pretty plots of the model predictions - predict for TONS of values, not just whatever's in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(0.5,20,100)\n",
    "\n",
    "predict_df = pd.DataFrame(data={\"age\":x_pred})\n",
    "predict_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `get_prediction(<data>).summary_frame()` to get the model's prediction (and error bars!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_output = fit1_lm.get_prediction(predict_df).summary_frame()\n",
    "prediction_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data, the fitted model, the **confidence intervals**, and the *prediction intervals*. For more on how `statsmodels` calculates these intervals see: https://www.statsmodels.org/stable/_modules/statsmodels/regression/_prediction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax1 = diab.plot.scatter(x='age',y='y',c='brown',title=\"Diabetes data with least-squares linear fit\")\n",
    "ax1.set_xlabel(\"Age at Diagnosis\")\n",
    "ax1.set_ylabel(\"Log C-Peptide Concentration\")\n",
    "\n",
    "ax1.plot(predict_df.age, prediction_output['mean'],color=\"green\")\n",
    "ax1.plot(predict_df.age, prediction_output['mean_ci_lower'], color=\"blue\",linestyle=\"dashed\")\n",
    "ax1.plot(predict_df.age, prediction_output['mean_ci_upper'], color=\"blue\",linestyle=\"dashed\")\n",
    "ax1.plot(predict_df.age, prediction_output['obs_ci_lower'], color=\"green\",linestyle=\"dashdot\")\n",
    "ax1.plot(predict_df.age, prediction_output['obs_ci_upper'], color=\"green\",linestyle=\"dashdot\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Breakout Room Exercise</b></div>\n",
    "\n",
    "- Fit a 3rd degree polynomial model to predict `y` using only `age` and\n",
    "- Plot the model and its confidence intervals.\n",
    "- Change the degree of your polynomial and see what happens to the fitted curve.\n",
    "- Does our model have an intercept? *Note*: we can discover the existence or not of an intercept in our model by running:\n",
    "```\n",
    "model_name.params\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"discussion\"><b>Vandermonde matrix in formulas</b></div>\n",
    "    \n",
    "It's easier to build higher order polynomials using `np.vandrer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"y ~ np.vander(age, 6, increasing=True) -1\" \n",
    "fit3_lm = smf.ols(formula=formula, data=diab).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit3_lm.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To our point why the Vandermonde matrix is usually ill-conditioned, \n",
    "# find the condition number of this matrix\n",
    "np.linalg.cond(np.vander(predict_df.age, 6, increasing=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution \n",
    "poly_predictions = fit3_lm.get_prediction(predict_df).summary_frame()\n",
    "poly_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "x_pred = np.linspace(0.5,15,100)\n",
    "predict_df = pd.DataFrame(data={\"age\":x_pred})\n",
    "\n",
    "ax2 = diab.plot.scatter(x='age',y='y',c='Red',title=\"Diabetes data with least-squares cubic fit\")\n",
    "ax2.set_xlabel(\"Age at Diagnosis\")\n",
    "ax2.set_ylabel(\"Log C-Peptide Concentration\")\n",
    "\n",
    "ax2.plot(predict_df.age, poly_predictions['mean'],color=\"green\")\n",
    "ax2.plot(predict_df.age, poly_predictions['mean_ci_lower'], color=\"blue\",linestyle=\"dashed\", label='confidence interval')\n",
    "ax2.plot(predict_df.age, poly_predictions['mean_ci_upper'], color=\"blue\",linestyle=\"dashed\");\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"discussion\"><b>Discussion</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QR decomposition (**Beyond the scope of this class**)\n",
    "\n",
    "As you know, to find the parameters of our model, we may try to solve the so-called *normal equations*, which, written in matrix form, are:<BR>\n",
    "\\begin{equation}\n",
    "    (\\boldsymbol{A^T}\\cdot \\boldsymbol{A}) \\cdot \\boldsymbol{b} = \\boldsymbol{A} \\cdot \\boldsymbol{y}\n",
    "\\end{equation}\n",
    "    \n",
    "The direct solution is $\\hat{\\boldsymbol{b}}=(\\boldsymbol{A}^T\\cdot \\boldsymbol{A})^{-1}\\cdot \\boldsymbol{A}^T \\cdot \\boldsymbol{y}$\n",
    "    \n",
    "Solving the least-squares problem directly via the normal equations is susceptible to roundoff error when the condition of the matrix $\\boldsymbol{A}$ is large. An alternative technique involves QR decomposition (details in any good linear algebra book). `statsmodels` lets you use this technique via a parameter in the `.fit`:\n",
    "```\n",
    "   .fit(method='qr') \n",
    "```\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try with QR now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"y ~ np.vander(age, 6, increasing=True) -1\" \n",
    "fit3_lm = smf.ols(formula=formula, data=diab).fit(method='qr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit3_lm.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution \n",
    "poly_predictions = fit3_lm.get_prediction(predict_df).summary_frame()\n",
    "poly_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "ax2 = diab.plot.scatter(x='age',y='y',c='Red',title=\"Diabetes data with least-squares cubic fit\")\n",
    "ax2.set_xlabel(\"Age at Diagnosis\")\n",
    "ax2.set_ylabel(\"Log C-Peptide Concentration\")\n",
    "\n",
    "ax2.plot(predict_df.age, poly_predictions['mean'],color=\"green\")\n",
    "ax2.plot(predict_df.age, poly_predictions['mean_ci_lower'], color=\"blue\",linestyle=\"dashed\", label='confidence interval')\n",
    "ax2.plot(predict_df.age, poly_predictions['mean_ci_upper'], color=\"blue\",linestyle=\"dashed\");\n",
    "ax2.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
